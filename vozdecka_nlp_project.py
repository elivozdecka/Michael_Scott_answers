# -*- coding: utf-8 -*-
"""PROJEKT_2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e8WFXY_L4rA1SEaqAwZbQZQRESe3lViI

#Intro
Installing Haystack and other required packages with pip
"""

!pip install haystack-ai
!pip install "datasets>=2.6.1"
!pip install "sentence-transformers>=4.1.0"
!pip install google-genai-haystack

"""Importing libraries"""

from haystack.document_stores.in_memory import InMemoryDocumentStore
from pathlib import Path
from haystack.components.converters import TextFileToDocument
from haystack import Document
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.builders import ChatPromptBuilder
from haystack.dataclasses import ChatMessage
import os
from getpass import getpass
from haystack_integrations.components.generators.google_genai import GoogleGenAIChatGenerator
from haystack import Pipeline
import time

#API key
os.environ["GOOGLE_API_KEY"] = "AIzaSyBHvHDQuEX9mTavAWVbebiUCJW1c1xG7RU"

#znepřístupnit po uveřejnění

from google.colab import drive
drive.mount('/content/drive')

"""#Fetching and Indexing Documents

Initializing the DocumentStore
"""

document_store = InMemoryDocumentStore()

#loading txt external dataset

converter = TextFileToDocument()

# expand all txt files
files = list(Path("/content/drive/MyDrive/NLP/Project/dataset_a").glob("*.txt"))

docs = converter.run(sources=files)
docs

"""Initalize a Document Embedder"""

doc_embedder = SentenceTransformersDocumentEmbedder(model="sentence-transformers/all-MiniLM-L6-v2")
doc_embedder.warm_up()

"""Write Documents to the DocumentStore"""

docs_with_embeddings = doc_embedder.run(docs["documents"])
document_store.write_documents(docs_with_embeddings["documents"])

"""#Building the RAG pipeline
Initialize a Text Embedder
"""

text_embedder = SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2") # using same model to make embedding as in doc_embedder

"""Initialize the Retriever"""

retriever = InMemoryEmbeddingRetriever(document_store)

"""Define a Template Prompt"""

template = [
    ChatMessage.from_user(
        """
Given the following information, answer the question.

Context:
{% for document in documents %}
    {{ document.content }}
{% endfor %}

Question: {{question}}
Answer:
"""
    )
]

prompt_builder = ChatPromptBuilder(template=template)

"""Initialize a ChatGenerator"""

chat_generator = GoogleGenAIChatGenerator(model="gemini-2.0-flash")

"""Build the Pipeline"""

basic_rag_pipeline = Pipeline()
# adding components to my pipeline
basic_rag_pipeline.add_component("text_embedder", text_embedder)
basic_rag_pipeline.add_component("retriever", retriever)
basic_rag_pipeline.add_component("prompt_builder", prompt_builder)
basic_rag_pipeline.add_component("llm", chat_generator)

# and connecting the components to each other
basic_rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
basic_rag_pipeline.connect("retriever", "prompt_builder")
basic_rag_pipeline.connect("prompt_builder.prompt", "llm.messages")

"""Asking a Question"""

questions = [
    "How much the company contribute to my health plan?",
    "How much paid off vacation time do I get?",
    "Who I am to report that I want to take vacation to?",
    "What are my working hours?",
    "Do I have to work on Christmas?",
    "Can I work from home?",
    "Do I get a working laptop or do Ineed to bring my personal one?",
    "Do I get a free coffee?",
    "Do i get a free lunch everyday?",
    "How many hours per week do I need to work?",
    "Do I get a parent leave even though I am not pregnant?",
    "How do I have to dress to work?",
    "Can I drink alcohol in the office?",
    "What should I do if I am feeling sick?",
    "How much time do I have for lunch breaks?",
    "Can I take unpaid time off?",
    "If I see some inappropriate behaviour in the office, who should I speak to?",
    "When I get my paycheck?",
    "Which document do I need to sign apart my contract?",
    "Do I get any free stuff?",
    "As a part time employee do I get also free lunch on fridays?",
    "Can I take my dog to the office?",
    "If I break my leg in the office, hom much do I receive for a workplace injury?",
    "Can I take my kids to work?",
    "Does the company provides daycare for toddlers?",
    "Are there any financial bonusses at the end of the year?",
    "Are my work travels reimburse?",
    "Can the company sponsor my VISA?",
    "Do you have a bunker in case of tornados?",
    "Can I recommend sombody for open positions?"
    ]

generated_answers = []

for q in questions:
    response = basic_rag_pipeline.run({
        "text_embedder": {"text": q},
        "prompt_builder": {"question": q}
    })
    answer = response["llm"]["replies"][0].text
    print(answer)
    generated_answers.append(answer)
    time.sleep(5)

generated_answers

with open("generated_answers.txt", "w") as f:
    for r in generated_answers:
        f.write(r + "\n")

len(generated_answers)

from google.colab import files

files.download("generated_answers.txt")   #

"""#Michael Scott styled
changing the Preambel in the template
"""

text_embedder_ms = SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2")

retriever_ms = InMemoryEmbeddingRetriever(document_store)

template_ms = [
    ChatMessage.from_user(
        """
Given the following information, answer the question as Michael Scott.

Context:
{% for document in documents %}
    {{ document.content }}
{% endfor %}

Question: {{question}}
Answer:
"""
    )
]

prompt_builder_ms = ChatPromptBuilder(template=template_ms)

chat_generator_ms = GoogleGenAIChatGenerator(model="gemini-2.0-flash")

basic_rag_pipeline_ms = Pipeline()
# adding components to my pipeline
basic_rag_pipeline_ms.add_component("text_embedder_ms", text_embedder_ms)
basic_rag_pipeline_ms.add_component("retriever_ms", retriever_ms)
basic_rag_pipeline_ms.add_component("prompt_builder_ms", prompt_builder_ms)
basic_rag_pipeline_ms.add_component("llm_ms", chat_generator_ms)

# and connecting the components to each other
basic_rag_pipeline_ms.connect("text_embedder_ms.embedding", "retriever_ms.query_embedding")
basic_rag_pipeline_ms.connect("retriever_ms", "prompt_builder_ms")
basic_rag_pipeline_ms.connect("prompt_builder_ms.prompt", "llm_ms.messages")

generated_answers_ms = []

for q in questions:
    response_ms = basic_rag_pipeline_ms.run({
        "text_embedder_ms": {"text": q},
        "prompt_builder_ms": {"question": q}
    })
    answer_ms = response_ms["llm_ms"]["replies"][0].text
    print(answer_ms)
    generated_answers_ms.append(answer_ms)
    time.sleep(5)

generated_answers_ms

with open("generated_answers_ms.txt", "w") as f:
    for r in generated_answers_ms:
        f.write(r + "\n")

files.download("generated_answers_ms.txt")

"""#Evaluate the Pipeline
semantic answer similarity evaluator
"""

from haystack.components.evaluators import SASEvaluator, FaithfulnessEvaluator

#ground truth answers that were manually looked up
with open("/content/drive/MyDrive/NLP/Project/ground_truth.txt", "r") as f:
    ground_truth = [line.strip() for line in f if line.strip()]

len(ground_truth)

ground_truth

sas_evaluator = SASEvaluator()
sas_evaluator.warm_up()
result = sas_evaluator.run(
  ground_truth_answers= ground_truth,
  predicted_answers=generated_answers
)

print(result["individual_scores"])

print(result["score"])

"""Faithfulness Evaluator"""

query_embedding = text_embedder.run(questions[0])["embedding"]

retrieved_docs = retriever.run(query_embedding=query_embedding, top_k=1)

query_embedding



from haystack.utils import Secret

google_generator = GoogleGenAIChatGenerator(
    api_key=Secret.from_token("AIzaSyBHvHDQuEX9mTavAWVbebiUCJW1c1xG7RU"),
    generation_kwargs={
        "response_mime_type": "application/json"  # forces JSON output
    }
)

faithfulness_evaluator = FaithfulnessEvaluator(chat_generator=google_generator)

pipeline_eval = Pipeline()
pipeline_eval.add_component("faithfulness_evaluator", faithfulness_evaluator)

questions_eval = questions
contexts_eval = []
for q in questions_eval:
    query_embedding = text_embedder.run(q)["embedding"]
    retrieved_docs = retriever.run(query_embedding=query_embedding, top_k=1)
    if retrieved_docs["documents"]:
        contexts_eval.append(retrieved_docs["documents"][0].content)
    else:
        contexts_eval.append("")

predicted_answers_eval = generated_answers

result_eval = pipeline_eval.run({
    "faithfulness_evaluator": {
        "questions": questions_eval,
        "contexts": contexts_eval,
        "predicted_answers": predicted_answers_eval
    }
})

print(result_eval["faithfulness_evaluator"]["results"])
print(result_eval["faithfulness_evaluator"]["score"])